{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f60a3907de1f4d74",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "<div style=\"display:flex; align-items:center; gap:10px;\">\n",
    "  <img src=\"ece_logo.png\" width=\"198\" height=\"91\" alt=\"ECE logo\" />\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dca7af484fc9e553",
   "metadata": {},
   "source": [
    "# **Rapport de projet – Analyse de données et apprentissage automatique**\n",
    "## Réalisé par les étudiants de B3 Data & IA :\n",
    "## _Kenza BELALOUI - Anis FETOUAB - Nathan BRUNET - Oleksandr KSHYVNYAK - Sirine BESSOUS_\n",
    "\n",
    "### Ce projet a pour objectif de conduire un pipeline complet d’analyse de données, depuis le choix du dataset jusqu’à l’application d’un ou plusieurs modèles de machine learning."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a57896a92314ed53",
   "metadata": {},
   "source": [
    "# Partie 1 – Définition du sujet et choix du dataset\n",
    "## 1.1 Thématique\n",
    "\n",
    "Notre projet se situe dans le domaine du sport, plus précisément l'analyse de données dans le football.\n",
    "\n",
    "L'objectif est de créer un modèle d'apprentissage automatique (machine learning) pour prédire si un joueur est un \"Top Attaquant\". Pour définir cette variable cible (TopAttacker), nous avons identifié les joueurs qui sont au-dessus de la médiane (la moyenne statistique) en GCA (Actions créant un but) et SCA (Actions créant un tir).\n",
    "\n",
    "Ce sujet mélange nos deux passions : le football et l'application concrète de la data science. Le défi était d'utiliser des statistiques avancées (au-delà des simples buts) pour modéliser une notion qui est d'habitude subjective.\n",
    "\n",
    "Ce projet sert concrètement au secteur sportif (clubs, médias) :\n",
    "\n",
    "1.\tAide au recrutement : Les clubs peuvent utiliser le modèle pour repérer des talents sous-évalués, en se basant sur des métriques avancées comme les GCA et SCA.\n",
    "2.\tAnalyse de performance : Les entraîneurs peuvent évaluer la contribution réelle d'un joueur à l'attaque, même s'il ne marque pas beaucoup.\n",
    "3.\tMédias et fans : Il fournit une base analytique pour comparer les joueurs et enrichir les débats.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc585e5fe258942c",
   "metadata": {},
   "source": [
    "## 1.2 Recherche et sélection du dataset\n",
    "### Informations générales sur le dataset :\n",
    "\n",
    "Nom du dataset :\n",
    "Source et lien d’accès :\n",
    "Auteur ou organisation :\n",
    "Taille (nombre de lignes et de colonnes) : 2690 lignes et 97 colonnes\n",
    "Format du fichier (CSV, JSON, Excel, etc.) : Fichier en .CSV\n",
    "\n",
    "### Vérification de la qualité :\n",
    "Le dataset est-il récent ?\n",
    "Les variables sont-elles clairement nommées et documentées ?\n",
    "Contient-il suffisamment de données (au moins plusieurs centaines de lignes) ?\n",
    "Le dataset comporte-t-il une variable cible que vous pourrez prédire ou expliquer ?\n",
    "Les données semblent-elles complètes et cohérentes ?\n",
    "\n",
    "### Justification du choix :\n",
    "Le fichier était idéal car il contenait les stats de GCA (Actions créant un But) et SCA (Actions créant un Tir). C'est crucial, car ces deux métriques nous ont permis de construire la variable cible (TopAttacker). Le reste des données (passes progressives, tirs, etc.) a servi de features (variables explicatives) pour entraîner le modèle.\n",
    "\n",
    "#### Avantages\n",
    "\n",
    "Richesse : Le dataset est hyper complet, avec des dizaines de métriques avancées comme PasProg (Passes Progressives) et CarProg (Portées Progressives), ce qui donne une description très fine du profil de chaque joueur.\n",
    "\n",
    "- Comparabilité : Toutes les données sont ramenées \"par 90 minutes\" (90s), ce qui garantit une comparaison équitable entre les joueurs, peu importe leur temps de jeu total.\n",
    "\n",
    "- Qualité : Les performances sont issues d'une compétition de haut niveau (Ligue des Champions), ce sont donc des données très pertinentes.\n",
    "\n",
    "#### Limites et Difficultés\n",
    "\n",
    "Nettoyage Technique : Le CSV a demandé un gros travail de preprocessing. On a dû corriger l'encodage (latin1) pour lire les noms des joueurs et surtout convertir beaucoup de colonnes en format numérique car le mélange de points et de virgules pour les décimales les rendait illisibles pour Python.\n",
    "\n",
    "Données Manquantes : Certaines lignes avaient des valeurs nulles (NaN). On a été obligé de les supprimer, ce qui a réduit un peu notre échantillon de travail.\n",
    "\n",
    "Déséquilibre des Classes : Le plus gros problème, c'est que nous avions peu de vrais \"Top Attaquants\" (classe 1) par rapport aux autres joueurs (classe 0). Ce dataset déséquilibré rend la tâche plus dure pour le modèle et nous oblige à utiliser des métriques plus solides que la simple accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d466a35545a81c61",
   "metadata": {},
   "source": [
    "## 1.3 Validation du dataset\n",
    "\n",
    "### Résultats du Modèle KNN (Test Set)\n",
    "\n",
    "| **Critères**      | **Questions**                                                    | **Réponses** | **Détail/Justification**\n",
    "|:------------------|:-----------------------------------------------------------------|:-------------|--------------------------|\n",
    "| **Pertinence**    | Le dataset permet-il de répondre à votre question de départ ?    | OUI          | **_A COMPLETER_**        |\n",
    "| **Clarté**        | Les variables sont-elles bien nommées et compréhensibles ?       | NON          | **_A COMPLETER_**        |\n",
    "| **Propreté**      | Les données semblent-elles utilisables sans nettoyage majeur ?   | NON          | **_A COMPLETER_**        |\n",
    "| **Taille**        | Le dataset est-il d’une taille adaptée à votre analyse ?         | OUI          | **_A COMPLETER_**        |\n",
    "| **Accessibilité** | Le format est-il compatible avec Python (CSV, XLSX) ?            | OUI          | **_A COMPLETER_**        |\n",
    "| **Actualité**     | Les données sont-elles récentes ou encore valides ?              | OUI          | **_A COMPLETER_**        |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d275ecac000f8",
   "metadata": {},
   "source": [
    "# Partie 2 – Exploration initiale des données\n",
    "## 2.1 Chargement et aperçu du dataset (Importez le dataset dans un notebook Python à l’aide de pandas. )\n",
    "\n",
    "#### Comment le dataset a-t-il été chargé ?\n",
    "\n",
    "On a utilisé la bibliothèque pandas pour lire le fichier champions-league-2024-UTC.csv. La commande clé était :\n",
    "\n",
    "df = pd.read_csv(csv_path, sep=';', decimal=',', index_col='Rk', encoding='latin1')\n",
    "\n",
    "L'astuce a été de spécifier sep=';' (point-virgule) et decimal=',' (virgule) pour le format européen, et surtout encoding='latin1' pour éviter les erreurs de lecture de caractères spéciaux dans les noms de joueurs.\n",
    "\n",
    "##### Nombre de lignes et de colonnes\n",
    "\n",
    "Après le chargement, le DataFrame contenait un certain nombre de lignes (joueurs) et de colonnes (statistiques). Ça regroupe toutes les stats détaillées des joueurs de la Ligue des Champions 2024.\n",
    "\n",
    "##### Principales variables (Features) clés\n",
    "\n",
    "- Statistiques de but : Goals (buts marqués), Shots (tirs), SoT (tirs cadrés).\n",
    "- Ratios : SoT% (pourcentage de tirs cadrés) et G/Sh, G/SoT (efficacité du tir).\n",
    "- Métriques de Créativité (les plus importantes) : GCA, SCA, PasProg (Passes Progressives) et CarProg (Portées Progressives). C'est sur ces dernières qu'on a basé notre classification.\n",
    "- Temps de jeu : 90s, qui est le temps total joué ramené à des matchs complets de 90 minutes.\n",
    "\n",
    "##### Gestion des valeurs manquantes / incohérences\n",
    "\n",
    "Nous avons dû gérer des valeurs manquantes (NaN) et des valeurs infinies (inf) dans certaines colonnes, notamment dans les ratios comme G/SoT. Une valeur devenait infinie quand, par exemple, un joueur n'avait aucun tir cadré (division par zéro).\n",
    "Pour nettoyer ça et garantir la cohérence des données avant de modéliser, on a appliqué deux étapes :\n",
    "<ol>\n",
    "<li>Remplacer toutes les valeurs infinies par NaN (valeur manquante) : df.replace([np.inf, -np.inf], np.nan, inplace=True)</li>\n",
    "<li>Remplacer ces NaN dans la colonne G/SoT par la médiane de la colonne (une valeur centrale) : df_filtered['G/SoT'] = df_filtered['G/SoT'].fillna(df_filtered['G/SoT'].median())</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "id": "10ceebba-c896-4ba2-bf70-37af7946f46c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T17:10:26.803014Z",
     "start_time": "2025-11-25T17:10:16.535152Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('champions-league-2024-UTC.csv', sep=';')\n",
    "df_attaquants = df[\n",
    "    (df['Pos'].str.contains('FW', na=False)) & \n",
    "    (df['Min'] >= 300)\n",
    "].copy()\n",
    "\n",
    "print(f\"Dataset initial : {df.shape}\")\n",
    "print(f\"Dataset filtré (Attaquants) : {df_attaquants.shape}\")"
   ],
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xf1 in position 7417: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mUnicodeDecodeError\u001B[39m                        Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 3\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpandas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpd\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m df = \u001B[43mpd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mchampions-league-2024-UTC.csv\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msep\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m;\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m      4\u001B[39m df_attaquants = df[\n\u001B[32m      5\u001B[39m     (df[\u001B[33m'\u001B[39m\u001B[33mPos\u001B[39m\u001B[33m'\u001B[39m].str.contains(\u001B[33m'\u001B[39m\u001B[33mFW\u001B[39m\u001B[33m'\u001B[39m, na=\u001B[38;5;28;01mFalse\u001B[39;00m)) & \n\u001B[32m      6\u001B[39m     (df[\u001B[33m'\u001B[39m\u001B[33mMin\u001B[39m\u001B[33m'\u001B[39m] >= \u001B[32m300\u001B[39m)\n\u001B[32m      7\u001B[39m ].copy()\n\u001B[32m      9\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mDataset initial : \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdf.shape\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001B[39m, in \u001B[36mread_csv\u001B[39m\u001B[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[39m\n\u001B[32m   1013\u001B[39m kwds_defaults = _refine_defaults_read(\n\u001B[32m   1014\u001B[39m     dialect,\n\u001B[32m   1015\u001B[39m     delimiter,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1022\u001B[39m     dtype_backend=dtype_backend,\n\u001B[32m   1023\u001B[39m )\n\u001B[32m   1024\u001B[39m kwds.update(kwds_defaults)\n\u001B[32m-> \u001B[39m\u001B[32m1026\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001B[39m, in \u001B[36m_read\u001B[39m\u001B[34m(filepath_or_buffer, kwds)\u001B[39m\n\u001B[32m    617\u001B[39m _validate_names(kwds.get(\u001B[33m\"\u001B[39m\u001B[33mnames\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[32m    619\u001B[39m \u001B[38;5;66;03m# Create the parser.\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m620\u001B[39m parser = \u001B[43mTextFileReader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    622\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mor\u001B[39;00m iterator:\n\u001B[32m    623\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001B[39m, in \u001B[36mTextFileReader.__init__\u001B[39m\u001B[34m(self, f, engine, **kwds)\u001B[39m\n\u001B[32m   1617\u001B[39m     \u001B[38;5;28mself\u001B[39m.options[\u001B[33m\"\u001B[39m\u001B[33mhas_index_names\u001B[39m\u001B[33m\"\u001B[39m] = kwds[\u001B[33m\"\u001B[39m\u001B[33mhas_index_names\u001B[39m\u001B[33m\"\u001B[39m]\n\u001B[32m   1619\u001B[39m \u001B[38;5;28mself\u001B[39m.handles: IOHandles | \u001B[38;5;28;01mNone\u001B[39;00m = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1620\u001B[39m \u001B[38;5;28mself\u001B[39m._engine = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_make_engine\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\pandas\\io\\parsers\\readers.py:1898\u001B[39m, in \u001B[36mTextFileReader._make_engine\u001B[39m\u001B[34m(self, f, engine)\u001B[39m\n\u001B[32m   1895\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(msg)\n\u001B[32m   1897\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1898\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmapping\u001B[49m\u001B[43m[\u001B[49m\u001B[43mengine\u001B[49m\u001B[43m]\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1899\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[32m   1900\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.handles \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:93\u001B[39m, in \u001B[36mCParserWrapper.__init__\u001B[39m\u001B[34m(self, src, **kwds)\u001B[39m\n\u001B[32m     90\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m kwds[\u001B[33m\"\u001B[39m\u001B[33mdtype_backend\u001B[39m\u001B[33m\"\u001B[39m] == \u001B[33m\"\u001B[39m\u001B[33mpyarrow\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m     91\u001B[39m     \u001B[38;5;66;03m# Fail here loudly instead of in cython after reading\u001B[39;00m\n\u001B[32m     92\u001B[39m     import_optional_dependency(\u001B[33m\"\u001B[39m\u001B[33mpyarrow\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m93\u001B[39m \u001B[38;5;28mself\u001B[39m._reader = \u001B[43mparsers\u001B[49m\u001B[43m.\u001B[49m\u001B[43mTextReader\u001B[49m\u001B[43m(\u001B[49m\u001B[43msrc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     95\u001B[39m \u001B[38;5;28mself\u001B[39m.unnamed_cols = \u001B[38;5;28mself\u001B[39m._reader.unnamed_cols\n\u001B[32m     97\u001B[39m \u001B[38;5;66;03m# error: Cannot determine type of 'names'\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mpandas/_libs/parsers.pyx:574\u001B[39m, in \u001B[36mpandas._libs.parsers.TextReader.__cinit__\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mpandas/_libs/parsers.pyx:663\u001B[39m, in \u001B[36mpandas._libs.parsers.TextReader._get_header\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mpandas/_libs/parsers.pyx:874\u001B[39m, in \u001B[36mpandas._libs.parsers.TextReader._tokenize_rows\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mpandas/_libs/parsers.pyx:891\u001B[39m, in \u001B[36mpandas._libs.parsers.TextReader._check_tokenize_status\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mpandas/_libs/parsers.pyx:2053\u001B[39m, in \u001B[36mpandas._libs.parsers.raise_parser_error\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m<frozen codecs>:325\u001B[39m, in \u001B[36mBufferedIncrementalDecoder.decode\u001B[39m\u001B[34m(self, input, final)\u001B[39m\n\u001B[32m    322\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mdecode\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m, final=\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[32m    323\u001B[39m     \u001B[38;5;66;03m# decode input (taking the buffer into account)\u001B[39;00m\n\u001B[32m    324\u001B[39m     data = \u001B[38;5;28mself\u001B[39m.buffer + \u001B[38;5;28minput\u001B[39m\n\u001B[32m--> \u001B[39m\u001B[32m325\u001B[39m     (result, consumed) = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_buffer_decode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfinal\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    326\u001B[39m     \u001B[38;5;66;03m# keep undecoded input until the next call\u001B[39;00m\n\u001B[32m    327\u001B[39m     \u001B[38;5;28mself\u001B[39m.buffer = data[consumed:]\n",
      "\u001B[31mUnicodeDecodeError\u001B[39m: 'utf-8' codec can't decode byte 0xf1 in position 7417: invalid continuation byte"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "8b2c31756b243e6e",
   "metadata": {},
   "source": [
    "## 2.2 Typologie des données Classification des variables\n",
    "\n",
    "__Quantitatives continues :__ Ce sont principalement les ratios ou les données lissées par le temps de jeu, comme le SoT% (précision), G/Sh (efficacité du tir), ShoDist (distance moyenne de tir) et le temps de jeu 90s.\n",
    "\n",
    "**Quantitatives discrètes :** Les décomptes bruts qui sont des nombres entiers, comme Goals, Shots, les métriques de créativité GCA et SCA, ainsi que PasProg et CarProg (passes et portées progressives).\n",
    "\n",
    "**Qualitatives nominales :** Les identifiants comme le nom du joueur, son club ou sa position sur le terrain. Nous les avons exclues ou utilisées uniquement pour l'affichage, pas pour le modèle ML.\n",
    "\n",
    "#### Variables les plus importantes pour l'analyse\n",
    "\n",
    "Les features clés pour notre analyse sont celles liées à la création d'occasions : GCA, SCA, PasProg et CarProg. Ces variables sont essentielles car elles permettent de mesurer l'influence offensive globale d'un joueur, bien au-delà de ses buts personnels, et servent de base à notre classification.\n",
    "\n",
    "#### Variable Cible :\n",
    "\n",
    "Oui, l'objectif principal du projet est de prédire notre variable cible : TopAttacker (qui est binaire : 1 ou 0).\n",
    "\n",
    "Définition technique : Nous avons créé cette cible en étiquetant un joueur 1 seulement si ses mesures de GCA_p90 (création de buts par 90 minutes) ET de SCA_p90 (création de tirs par 90 minutes) sont toutes deux au-dessus de la médiane des joueurs du dataset. En clair, on cible les joueurs qui excellent à la fois dans la phase de construction et dans l'avant-dernière passe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc7240cf2ac3d45",
   "metadata": {},
   "source": [
    "# Partie 3 – Nettoyage et préparation du dataset\n",
    "\n",
    "## 3.1 Gestion des valeurs manquantes\n",
    "\n",
    "Après la conversion des données en numérique, les deux principales colonnes qui avaient des problèmes étaient G/SoT (Buts par Tir Cadré) et SoT% (Pourcentage de Tirs Cadrés). Ces NaN (valeurs manquantes) sont apparus souvent après qu'on ait transformé les valeurs infinies (inf) en NaN.\n",
    "\n",
    "Notre stratégie a été l'imputation par la médiane :\n",
    "\n",
    "```Python\n",
    "df_numeric = df_numeric.fillna(df_numeric.median())\n",
    "```\n",
    "En résumé, nous avons remplacé chaque valeur manquante dans une colonne par la valeur médiane (la valeur centrale) de cette même colonne.\n",
    "\n",
    "Nous avons choisi la médiane plutôt que la moyenne pour une raison technique : la médiane est moins sensible aux valeurs extrêmes (outliers).\n",
    "\n",
    "Comme les ratios d'efficacité (comme G/SoT) peuvent avoir des valeurs très élevées ou très basses pour certains joueurs (ce qui fausserait la moyenne), la médiane donne une estimation plus robuste et représentative de la performance typique de l'ensemble des joueurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac6e856-b0a0-48e1-b61a-943e77d532e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_keep = [\n",
    "    'Player', 'Squad', 'Age',       # Identité\n",
    "    'Goals', 'Shots', 'SoT',        # Finition brute\n",
    "    'SoT%', 'Assists',              # Précision et Altruisme\n",
    "    'SCA', 'GCA',                   # Création (Shot/Goal Creating Actions)\n",
    "    'TouAttPen',                    # Présence : Touches dans la surface de réparation\n",
    "    'CarProg'                       # Percussion : Conduites de balle progressives\n",
    "]\n",
    "\n",
    "df_final = df_attaquants[cols_to_keep].copy()\n",
    "\n",
    "rename_dict = {\n",
    "    'Player': 'Joueur',\n",
    "    'Squad': 'Equipe',\n",
    "    'Goals': 'Buts',\n",
    "    'Shots': 'Tirs_Total',\n",
    "    'SoT': 'Tirs_Cadres',\n",
    "    'SoT%': 'Tirs_Cadres_Pct',\n",
    "    'Assists': 'Passes_Decisives',\n",
    "    'SCA': 'Actions_Creation_Tir',\n",
    "    'GCA': 'Actions_Creation_But',\n",
    "    'TouAttPen': 'Touches_Surface',\n",
    "    'CarProg': 'Percussions_Progressives'\n",
    "}\n",
    "\n",
    "df_final = df_final.rename(columns=rename_dict)\n",
    "df_final = df_final.reset_index(drop=True)\n",
    "display(df_final.head())\n",
    "\n",
    "df_final.to_csv('top_attaquants_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d09087898a83b",
   "metadata": {},
   "source": [
    "## 3.2 Détection et traitement des doublons\n",
    "\n",
    "Techniquement, nous n'avons pas trouvé de doublons, car nous n'avons pas exécuté de vérification explicite (df.duplicated()) sur l'ensemble du DataFrame.\n",
    "\n",
    "Toutefois, dans le contexte de ce projet, un tel contrôle n'a pas été jugé prioritaire. Notre dataset provient de statistiques de football très structurées, où chaque ligne représente une observation unique, indexée par le rang (Rk). Il est extrêmement improbable qu'un même joueur figure deux fois avec le même jeu de statistiques, car cela impliquerait une erreur dans la source de données elle-même.\n",
    "**Aucun traitement spécifique** n'a été nécessaire.\n",
    "\n",
    "Nous avons considéré que l'unicité des joueurs, garantie par la colonne _Player_ et l'index _Rk_, était suffisante.\n",
    "Si nous avions trouvé des doublons, la stratégie standard dans le preprocessing des données aurait été de les identifier et de les supprimer immédiatement pour éviter de biaiser l'apprentissage du modèle :\n",
    "Dans notre cas, nous nous sommes concentrés sur les étapes de nettoyage les plus cruciales pour le KNN : le traitement des valeurs manquantes et la normalisation des variables, qui étaient des problèmes bien plus critiques pour le bon fonctionnement de notre algorithme."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c3b3a7f09a081d",
   "metadata": {},
   "source": [
    "## 3.3 Détection des valeurs aberrantes\n",
    "\n",
    "Des valeurs extrêmes ont été identifiées dans des colonnes basées sur des ratios, comme _G/SoT_ ou _SoT%_.\n",
    "\n",
    "Ces valeurs ne sont pas considérées comme des erreurs de données, mais comme des cas particuliers qui reflètent la réalité sportive (par exemple, un joueur très efficace sur un très faible nombre de tirs).\n",
    "\n",
    "La justification est double :\n",
    "<ol>\n",
    "<li>Elles représentent le profil de performance exceptionnelle que notre variable cible _(TopAttacker)_ cherche justement à isoler.</li>\n",
    "\n",
    "<li>L'étape de normalisation (Standardisation) appliquée plus tard au dataset minimise l'influence disproportionnée de ces outliers sur le calcul des distances de l'algorithme KNN, rendant le modèle plus robuste.</li>\n",
    "</ol>\n",
    "\n",
    "Les seules corrections appliquées concernaient les valeurs infinies _(inf)_ qui rendaient le modèle inutilisable ; elles ont été traitées pour assurer la cohérence numérique du jeu de données."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5437cf627f8c9df6",
   "metadata": {},
   "source": [
    "## 3.4 Encodage et mise à l’échelle des variables\n",
    "\n",
    "Aucun encodage n'a été nécessaire. Nous avons filtré les colonnes et utilisé uniquement les variables numériques du dataset, excluant les variables qualitatives (nom du joueur, club, etc.) de l'entraînement.\n",
    "\n",
    "La méthode utilisée pour la mise à l'échelle est la Standardisation, implémentée avec la classe _StandardScaler_ de _scikit-learn_. Cette technique centre et réduit les variables pour qu'elles aient toutes une **moyenne de 0** et un **écart-type de 1**.\n",
    "\n",
    "La standardisation est une étape obligatoire et cruciale pour l'algorithme des K-Plus Proches Voisins (KNN) que nous avons sélectionné :\n",
    "<ol>\n",
    "<li>Distance Euclidienne : Le KNN fonctionne en calculant la distance euclidienne entre les joueurs. Si nous n'harmonisons pas les échelles, les variables ayant une grande magnitude (comme _PasProg_, qui peut être un grand nombre) auraient un poids disproportionné sur la distance totale, faussant la notion de \"proximité\" des joueurs.</li>\n",
    "\n",
    "<li>Comparabilité : Elle garantit que toutes les features (buts, tirs, créativité, etc.) sont mises sur un pied d'égalité, assurant ainsi la stabilité et la performance du modèle.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f7b634c3ca30cb",
   "metadata": {},
   "source": [
    "# Partie 4 – Analyse exploratoire et visualisations\n",
    "\n",
    "#### 1. **Corrélations et Relations Observées :**\n",
    " Nous avons observé une corrélation positive marquée entre les métriques de création d'occasions (SCA - Actions de Création de Tir) et l'efficacité pure de la finition.\n",
    "- Ca signifie qu'un joueur très impliqué dans la construction offensive (celui qui initie, dribble ou passe avant un tir) est statistiquement plus susceptible d'être performant à la finition. Cela tend à prouver que le Top Attaquant moderne ne se limite pas à marquer, il doit être un facteur d'influence global sur l'approche de la surface adverse.\n",
    "\n",
    "#### 2. **Influence sur la Variable Cible :**\n",
    "L'analyse de l'importance des features (post-modélisation) est très claire : ce ne sont pas uniquement les buts qui discriminent nos classes. Les variables qui décrivent la progression du jeu sont primordiales.\n",
    "- Les métriques comme SCA, Percussions_Progressives (CarProg) et les passes progressives sont en tête de liste. Elles possèdent la plus forte capacité discriminante pour le modèle KNN, ce qui signifie que ces actions sont déterminantes pour classer un joueur dans la catégorie \"Top Attaquant\".\n",
    "\n",
    "#### 3. **Hypothèses et Tendances Graphiques :**\n",
    "Notre première hypothèse est que le succès d'un attaquant est multifactoriel. La classification ne dépend pas d'un seul pic de statistique, mais d'une répartition équilibrée de la performance sur plusieurs axes (finition, création, progression).\n",
    "- Si on regarde nos tendances visuelles le Quadrant Plot (Création vs. Finition) confirme cette hypothèse en regroupant nos meilleurs joueurs dans le quadrant de \"Haute Création et Haute Efficacité\". Ils sont à la fois créateurs et finisseurs.\n",
    "\n",
    "- Le Radar Plot renforce cette idée en montrant que le profil moyen des \"Top Finisseurs\" présente une enveloppe statistique plus large et homogène sur toutes les features comparées aux autres joueurs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
